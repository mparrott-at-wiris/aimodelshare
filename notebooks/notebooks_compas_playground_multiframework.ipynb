{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "name": "compas_playground_multiframework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "CPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mparrott-at-wiris/aimodelshare/blob/master/notebooks/compas_playground_multiframework.ipynb)\n",
    "\n",
    "# COMPAS Multi-Framework Playground (Lightweight)\n",
    "This notebook replicates (in interactive form) the lightweight CI test logic found in `tests/test_playground_compas_multiframework_short.py`.\n",
    "\n",
    "It will:\n",
    "1. Configure `aimodelshare` credentials (interactive prompt).\n",
    "2. Load & preprocess the ProPublica COMPAS two-year recidivism dataset (sampled to 2,500 rows for efficiency).\n",
    "3. Create a private (optionally public) `ModelPlayground` for a classification task.\n",
    "4. Train and submit minimal models across frameworks:\n",
    "   - Scikit-learn: Logistic Regression, Random Forest\n",
    "   - Keras: Simple Sequential Dense Network\n",
    "   - PyTorch: Basic MLP\n",
    "5. Attach custom metadata field `Moral_Compass_Fairness` cycling through values (0.25, 0.50, 0.75) per submission.\n",
    "6. Display leaderboard and validate tag presence.\n",
    "\n",
    "Run cells in order. If ONNX or stdin-related export issues occur (sometimes in constrained environments), those submissions are skipped gracefully.\n",
    "\n",
    "**NOTE:** For real usage, ensure you have valid AWS and platform credentials. In Colab you may paste them directly when prompted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install / Upgrade Dependencies\n",
    "If running in a fresh Colab environment, install (or upgrade) the required packages."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install-deps"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install --quiet aimodelshare scikit-learn tensorflow torch pandas requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Global Configuration\n",
    "Set random seeds for reproducibility; define constants & feature lists matching the test script."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "imports"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "from getpass import getpass\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from aimodelshare.playground import ModelPlayground\n",
    "from aimodelshare.aws import configure_credentials, set_credentials, get_aws_token\n",
    "from aimodelshare.modeluser import get_jwt_token, create_user_getkeyandpassword\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Dataset configuration\n",
    "MAX_ROWS = 2500\n",
    "TOP_N_CHARGE_CATEGORIES = 50\n",
    "\n",
    "# Feature sets (align with test file constants)\n",
    "NUMERIC_FEATURES = ['age', 'priors_count', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'days_b_screening_arrest']\n",
    "CATEGORICAL_FEATURES = ['race', 'sex', 'age_cat', 'c_charge_degree', 'c_charge_desc']\n",
    "\n",
    "def fairness_value_generator():\n",
    "    \"\"\"Cycle through fairness values for custom metadata submissions.\"\"\"\n",
    "    return itertools.cycle([0.25, 0.50, 0.75])\n",
    "\n",
    "def build_custom_metadata(fairness_value: float) -> dict:\n",
    "    return {\"Moral_Compass_Fairness\": f\"{fairness_value:.2f}\"}\n",
    "\n",
    "print(\"Imports and globals initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Credentials\n",
    "You will be prompted for:\n",
    "- Platform username & password\n",
    "- AWS Access Key ID\n",
    "- AWS Secret Access Key\n",
    "- AWS Region\n",
    "\n",
    "They will be stored temporarily in a local `credentials.txt` file and set for deployment. If you've already configured credentials in this environment you may skip re-running.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "credentials"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Configure aimodelshare credentials (follow prompts):\")\n",
    "configure_credentials()  # interactive prompts\n",
    "set_credentials(credential_file=\"credentials.txt\", type=\"deploy_model\")\n",
    "\n",
    "try:\n",
    "    # Attempt to acquire tokens (optional validation)\n",
    "    aws_token = get_aws_token()\n",
    "    print(\"AWS token retrieved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not retrieve AWS token: {e}\")\n",
    "\n",
    "try:\n",
    "    # Validate JWT tokens; create user key/password if needed\n",
    "    # (If user already exists, create_user_getkeyandpassword may warn; that's acceptable.)\n",
    "    username = os.environ.get('username') or input(\"Re-enter platform username (for JWT test): \")\n",
    "    password = os.environ.get('password') or getpass(\"Re-enter platform password (hidden): \")\n",
    "    get_jwt_token(username, password)\n",
    "    create_user_getkeyandpassword()\n",
    "    print(\"JWT validation succeeded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: JWT validation issue: {e}\")\n",
    "\n",
    "print(\"Credentials configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load & Preprocess COMPAS Data\n",
    "Mirrors logic from the test file: download, sample, feature engineer charge description categories, split, and construct preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "data-prep"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "COMPAS_URL = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "response = requests.get(COMPAS_URL)\n",
    "df = pd.read_csv(StringIO(response.text))\n",
    "print(f\"Downloaded COMPAS dataset: {df.shape}\")\n",
    "\n",
    "# Sample for performance\n",
    "if df.shape[0] > MAX_ROWS:\n",
    "    df = df.sample(n=MAX_ROWS, random_state=42)\n",
    "    print(f\"Sampled to {MAX_ROWS} rows\")\n",
    "\n",
    "feature_columns = [\n",
    "    'race', 'sex', 'age', 'age_cat', 'c_charge_degree', 'c_charge_desc',\n",
    "    'priors_count', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'days_b_screening_arrest'\n",
    "]\n",
    "target_column = 'two_year_recid'\n",
    "\n",
    "# Condense c_charge_desc to top-N + OTHER_DESC\n",
    "if 'c_charge_desc' in df.columns:\n",
    "    top_charges = df['c_charge_desc'].value_counts().head(TOP_N_CHARGE_CATEGORIES).index\n",
    "    df['c_charge_desc'] = df['c_charge_desc'].apply(\n",
    "        lambda x: x if pd.notna(x) and x in top_charges else 'OTHER_DESC'\n",
    "    )\n",
    "\n",
    "X = df[feature_columns].copy()\n",
    "y = df[target_column].values\n",
    "print(f\"Features shape: {X.shape}; Target distribution: {pd.Series(y).value_counts().to_dict()}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Train shape: {X_train.shape}; Test shape: {X_test.shape}\")\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, NUMERIC_FEATURES),\n",
    "    ('cat', categorical_transformer, CATEGORICAL_FEATURES)\n",
    "])\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "def preprocessor_func(data):\n",
    "    return preprocessor.transform(data)\n",
    "\n",
    "print(\"Preprocessing pipeline fitted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create ModelPlayground\n",
    "We create a classification playground, using the test labels as evaluation data. Set `public=True` if you want it discoverable (optional)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "playground-create"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_labels = list(y_test)\n",
    "playground = ModelPlayground(input_type='tabular', task_type='classification', private=True)\n",
    "playground.create(eval_data=eval_labels, public=True)\n",
    "print(\"Playground created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Helper Function for Submissions\n",
    "Handles metadata, PyTorch dummy input creation, and ONNX/stdin error skipping."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "helper-function"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "def submit_model_helper(playground, model, preprocessor_obj, preds, framework, model_name, submission_type, fairness_value):\n",
    "    try:\n",
    "        extra_kwargs = {}\n",
    "        if framework == 'pytorch':\n",
    "            # Build dummy input after preprocessing a single synthetic row\n",
    "            dummy_data = {feat: [0] for feat in NUMERIC_FEATURES}\n",
    "            dummy_data.update({feat: ['A'] for feat in CATEGORICAL_FEATURES})\n",
    "            X_dummy = pd.DataFrame(dummy_data)\n",
    "            X_processed = preprocessor_obj.transform(X_dummy)\n",
    "            input_dim = X_processed.shape[1]\n",
    "            dummy_input = torch.zeros((1, input_dim), dtype=torch.float32)\n",
    "            extra_kwargs['model_input'] = dummy_input\n",
    "\n",
    "        custom_metadata = build_custom_metadata(fairness_value)\n",
    "        print(f\"Submitting {model_name} ({framework}) as {submission_type} with metadata: {custom_metadata}\")\n",
    "\n",
    "        playground.submit_model(\n",
    "            model=model,\n",
    "            preprocessor=preprocessor_obj,\n",
    "            prediction_submission=preds,\n",
    "            input_dict={\n",
    "                'description': f'Notebook submission {framework} {model_name} COMPAS_short {submission_type}',\n",
    "                'tags': f'compas_short,{framework},{submission_type}'\n",
    "            },\n",
    "            submission_type=submission_type,\n",
    "            custom_metadata=custom_metadata,\n",
    "            **extra_kwargs\n",
    "        )\n",
    "        print(\"✓ Submission succeeded.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        error_lower = str(e).lower()\n",
    "        if 'stdin' in error_lower or 'onnx' in error_lower:\n",
    "            print(f\"⊘ Skipped {model_name} due to ONNX/stdin export issue: {e}\")\n",
    "            return False\n",
    "        print(f\"✗ Submission failed: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train & Submit Scikit-learn Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sklearn-submit"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "fairness_gen = fairness_value_generator()\n",
    "\n",
    "X_train_processed = preprocessor_func(X_train)\n",
    "X_test_processed = preprocessor_func(X_test)\n",
    "\n",
    "sklearn_models = [\n",
    "    (\"LogisticRegression\", LogisticRegression(max_iter=500, random_state=42, class_weight='balanced')),\n",
    "    (\"RandomForestClassifier\", RandomForestClassifier(n_estimators=40, max_depth=10, random_state=42, class_weight='balanced')),\n",
    "]\n",
    "\n",
    "for name, model in sklearn_models:\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\"Training {name}\")\n",
    "    try:\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "            preds = (proba >= 0.5).astype(int)\n",
    "        else:\n",
    "            preds = model.predict(X_test_processed)\n",
    "        print(f\"Predictions generated: {len(preds)}; Distribution: {pd.Series(preds).value_counts().to_dict()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Training failed for {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    for submission_type in ['competition', 'experiment']:\n",
    "        fairness_val = next(fairness_gen)\n",
    "        submit_model_helper(playground, model, preprocessor, preds, 'sklearn', name, submission_type, fairness_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train & Submit Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "keras-submit"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Keras Sequential Model\")\n",
    "input_dim = X_train_processed.shape[1]\n",
    "\n",
    "keras_model = Sequential([\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "keras_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "try:\n",
    "    keras_model.fit(X_train_processed, y_train, epochs=6, batch_size=64, verbose=0, validation_split=0.1)\n",
    "    proba = keras_model.predict(X_test_processed, verbose=0).flatten()\n",
    "    keras_preds = (proba >= 0.5).astype(int)\n",
    "    print(f\"Keras predictions distribution: {pd.Series(keras_preds).value_counts().to_dict()}\")\n",
    "    for submission_type in ['competition', 'experiment']:\n",
    "        fairness_val = next(fairness_gen)\n",
    "        submit_model_helper(playground, keras_model, preprocessor, keras_preds, 'keras', 'sequential_dense', submission_type, fairness_val)\n",
    "except Exception as e:\n",
    "    print(f\"✗ Keras training/submission failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train & Submit PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pytorch-submit"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training PyTorch MLP Model\")\n",
    "input_dim = X_train_processed.shape[1]\n",
    "\n",
    "class MLPBasic(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "pytorch_model = MLPBasic(input_dim)\n",
    "\n",
    "try:\n",
    "    X_train_tensor = torch.FloatTensor(X_train_processed)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_processed)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=0.001)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    pytorch_model.train()\n",
    "    for epoch in range(6):\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = pytorch_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    pytorch_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = pytorch_model(X_test_tensor)\n",
    "        proba = torch.sigmoid(logits).numpy().flatten()\n",
    "        pytorch_preds = (proba >= 0.5).astype(int)\n",
    "    print(f\"PyTorch predictions distribution: {pd.Series(pytorch_preds).value_counts().to_dict()}\")\n",
    "\n",
    "    for submission_type in ['competition', 'experiment']:\n",
    "        fairness_val = next(fairness_gen)\n",
    "        submit_model_helper(playground, pytorch_model, preprocessor, pytorch_preds, 'pytorch', 'mlp_basic', submission_type, fairness_val)\n",
    "except Exception as e:\n",
    "    print(f\"✗ PyTorch training/submission failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Retrieve & Inspect Leaderboard\n",
    "Ensures submissions exist and prints full leaderboard for review."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "leaderboard"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    lb_data = playground.get_leaderboard()\n",
    "    if isinstance(lb_data, dict):\n",
    "        df_lb = pd.DataFrame(lb_data)\n",
    "    else:\n",
    "        df_lb = lb_data\n",
    "\n",
    "    if df_lb.empty:\n",
    "        print(\"Leaderboard is empty. (Possibly all submissions failed or were skipped.)\")\n",
    "    else:\n",
    "        print(f\"Leaderboard entries: {len(df_lb)}\")\n",
    "        # Tag checks\n",
    "        if 'tags' in df_lb.columns:\n",
    "            tag_series = df_lb['tags'].astype(str)\n",
    "            print(\"Tag counts:\")\n",
    "            for t in ['compas_short', 'sklearn', 'keras', 'pytorch', 'competition', 'experiment']:\n",
    "                print(f\"  {t}: {tag_series.str.contains(t, case=False, na=False).sum()}\")\n",
    "\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1200):\n",
    "            print(df_lb.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"Failed to retrieve leaderboard: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps\n",
    "- Adjust model architectures or hyperparameters for experimentation.\n",
    "- Use different fairness metadata strategies.\n",
    "- Toggle playground visibility or extend with new frameworks.\n",
    "- Integrate automated evaluation workflows.\n",
    "\n",
    "This concludes the lightweight multi-framework COMPAS submission demo."
   ]
  }
 ]
}