{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "name": "compas_playground_multiframework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "CPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mparrott-at-wiris/aimodelshare/blob/master/notebooks/notebooks_compas_playground_multiframework.ipynb)\n",
    "\n",
    "# COMPAS Multi-Framework Playground (Lightweight)\n",
    "This notebook replicates (in interactive form) the lightweight CI test logic found in `tests/test_playground_compas_multiframework_short.py`.\n",
    "\n",
    "It will:\n",
    "1. Configure `aimodelshare` credentials (interactive prompt).\n",
    "2. Load & preprocess the ProPublica COMPAS two-year recidivism dataset (sampled to 2,500 rows for efficiency).\n",
    "3. Create a private (optionally public) `ModelPlayground` for a classification task.\n",
    "4. Train and submit minimal models across frameworks:\n",
    "   - Scikit-learn: Logistic Regression, Random Forest\n",
    "   - Keras: Simple Sequential Dense Network\n",
    "   - PyTorch: Basic MLP\n",
    "5. Attach custom metadata field `Moral_Compass_Fairness` cycling through values (0.25, 0.50, 0.75) per submission.\n",
    "6. Display leaderboard and validate tag presence.\n",
    "\n",
    "Run cells in order. If ONNX or stdin-related export issues occur (sometimes in constrained environments), those submissions are skipped gracefully.\n",
    "\n",
    "**NOTE:** For real usage, ensure you have valid AWS and platform credentials. In Colab you may paste them directly when prompted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install / Upgrade Dependencies\n",
    "If running in a fresh Colab environment, install (or upgrade) the required packages."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install-deps"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install --quiet aimodelshare scikit-learn tensorflow torch pandas requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Global Configuration\n",
    "Set random seeds for reproducibility; define constants & feature lists matching the test script."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "imports"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "from getpass import getpass\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from aimodelshare.playground import ModelPlayground\n",
    "from aimodelshare.aws import configure_credentials, set_credentials, get_aws_token\n",
    "from aimodelshare.modeluser import get_jwt_token, create_user_getkeyandpassword\n",
    "\n",
    "# Moral Compass imports added later for challenge section\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Dataset configuration\n",
    "MAX_ROWS = 2500\n",
    "TOP_N_CHARGE_CATEGORIES = 50\n",
    "\n",
    "# Feature sets (align with test file constants)\n",
    "NUMERIC_FEATURES = ['age', 'priors_count', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'days_b_screening_arrest']\n",
    "CATEGORICAL_FEATURES = ['race', 'sex', 'age_cat', 'c_charge_degree', 'c_charge_desc']\n",
    "\n",
    "def fairness_value_generator():\n",
    "    \"\"\"Cycle through fairness values for custom metadata submissions.\"\"\"\n",
    "    return itertools.cycle([0.25, 0.50, 0.75])\n",
    "\n",
    "def build_custom_metadata(fairness_value: float) -> dict:\n",
    "    return {\"Moral_Compass_Fairness\": f\"{fairness_value:.2f}\"}\n",
    "\n",
    "print(\"Imports and globals initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Credentials\n",
    "You will be prompted for:\n",
    "- Platform username & password\n",
    "- AWS Access Key ID\n",
    "- AWS Secret Access Key\n",
    "- AWS Region\n",
    "\n",
    "They will be stored temporarily in a local `credentials.txt` file and set for deployment. If you've already configured credentials in this environment you may skip re-running.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "credentials"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Configure aimodelshare credentials (follow prompts):\")\n",
    "configure_credentials()  # interactive prompts\n",
    "set_credentials(credential_file=\"credentials.txt\", type=\"deploy_model\")\n",
    "\n",
    "try:\n",
    "    # Attempt to acquire tokens (optional validation)\n",
    "    aws_token = get_aws_token()\n",
    "    print(\"AWS token retrieved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not retrieve AWS token: {e}\")\n",
    "\n",
    "try:\n",
    "    # Validate JWT tokens; create user key/password if needed\n",
    "    username = os.environ.get('username') or input(\"Re-enter platform username (for JWT test): \")\n",
    "    password = os.environ.get('password') or getpass(\"Re-enter platform password (hidden): \")\n",
    "    get_jwt_token(username, password)\n",
    "    print(\"JWT validation succeeded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: JWT validation issue: {e}\")\n",
    "\n",
    "print(\"Credentials configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Ensure JWT Token for Moral Compass API\n",
    "Explicitly ensure JWT_AUTHORIZATION_TOKEN is available for downstream moral compass API usage."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ensure-jwt-token"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ensure JWT_AUTHORIZATION_TOKEN is set for moral compass API usage\n",
    "if not os.getenv('JWT_AUTHORIZATION_TOKEN'):\n",
    "    print(\"JWT_AUTHORIZATION_TOKEN not found in environment. Attempting to generate...\")\n",
    "    username = os.getenv('username') or os.getenv('AIMODELSHARE_USERNAME')\n",
    "    password = os.getenv('password') or os.getenv('AIMODELSHARE_PASSWORD')\n",
    "    \n",
    "    if username and password:\n",
    "        try:\n",
    "            # Generate JWT token (this sets JWT_AUTHORIZATION_TOKEN in environment)\n",
    "            get_jwt_token(username, password)\n",
    "            jwt_token = os.getenv('JWT_AUTHORIZATION_TOKEN')\n",
    "            if jwt_token:\n",
    "                print(f\"✓ JWT token generated successfully. Token: {jwt_token[:10]}...\")\n",
    "            else:\n",
    "                print(\"⚠️ JWT generation completed but token not found in environment\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to generate JWT token: {e}\")\n",
    "            print(\"Moral compass functionality may require manual JWT_AUTHORIZATION_TOKEN setup\")\n",
    "    else:\n",
    "        print(\"⚠️ No username/password credentials found for JWT generation\")\n",
    "        print(\"Set AIMODELSHARE_USERNAME and AIMODELSHARE_PASSWORD or JWT_AUTHORIZATION_TOKEN for moral compass API\")\n",
    "else:\n",
    "    jwt_token = os.getenv('JWT_AUTHORIZATION_TOKEN')\n",
    "    print(f\"✓ JWT_AUTHORIZATION_TOKEN already set. Token: {jwt_token[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load & Preprocess COMPAS Data\n",
    "Mirrors logic from the test file: download, sample, feature engineer charge description categories, split, and construct preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "data-prep"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "COMPAS_URL = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "response = requests.get(COMPAS_URL)\n",
    "df = pd.read_csv(StringIO(response.text))\n",
    "print(f\"Downloaded COMPAS dataset: {df.shape}\")\n",
    "\n",
    "# Sample for performance\n",
    "if df.shape[0] > MAX_ROWS:\n",
    "    df = df.sample(n=MAX_ROWS, random_state=42)\n",
    "    print(f\"Sampled to {MAX_ROWS} rows\")\n",
    "\n",
    "feature_columns = [\n",
    "    'race', 'sex', 'age', 'age_cat', 'c_charge_degree', 'c_charge_desc',\n",
    "    'priors_count', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'days_b_screening_arrest'\n",
    "]\n",
    "target_column = 'two_year_recid'\n",
    "\n",
    "# Condense c_charge_desc to top-N + OTHER_DESC\n",
    "if 'c_charge_desc' in df.columns:\n",
    "    top_charges = df['c_charge_desc'].value_counts().head(TOP_N_CHARGE_CATEGORIES).index\n",
    "    df['c_charge_desc'] = df['c_charge_desc'].apply(\n",
    "        lambda x: x if pd.notna(x) and x in top_charges else 'OTHER_DESC'\n",
    "    )\n",
    "\n",
    "X = df[feature_columns].copy()\n",
    "y = df[target_column].values\n",
    "print(f\"Features shape: {X.shape}; Target distribution: {pd.Series(y).value_counts().to_dict()}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Train shape: {X_train.shape}; Test shape: {X_test.shape}\")\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, NUMERIC_FEATURES),\n",
    "    ('cat', categorical_transformer, CATEGORICAL_FEATURES)\n",
    "])\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "def preprocessor_func(data):\n",
    "    return preprocessor.transform(data)\n",
    "\n",
    "print(\"Preprocessing pipeline fitted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create ModelPlayground\n",
    "We create a classification playground, using the test labels as evaluation data. Set `public=True` if you want it discoverable (optional)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "playground-create"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_labels = list(y_test)\n",
    "playground = ModelPlayground(input_type='tabular', task_type='classification', private=True)\n",
    "playground.create(eval_data=eval_labels, public=True)\n",
    "print(\"Playground created.\")\n",
    "try:\n",
    "    playground_id = getattr(playground, 'playground_id', None) or getattr(playground, 'id', None)\n",
    "    print(f\"Playground ID: {playground_id}\")\n",
    "except Exception:\n",
    "    playground_id = None\n",
    "    print(\"Could not access playground ID attribute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Helper Function for Submissions\n",
    "Handles metadata, PyTorch dummy input creation, and ONNX/stdin error skipping."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "helper-function"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "def submit_model_helper(playground, model, preprocessor_obj, preds, framework, model_name, submission_type, fairness_value):\n",
    "    try:\n",
    "        extra_kwargs = {}\n",
    "        if framework == 'pytorch':\n",
    "            # Build dummy input after preprocessing a single synthetic row\n",
    "            dummy_data = {feat: [0] for feat in NUMERIC_FEATURES}\n",
    "            dummy_data.update({feat: ['A'] for feat in CATEGORICAL_FEATURES})\n",
    "            X_dummy = pd.DataFrame(dummy_data)\n",
    "            X_processed = preprocessor_obj.transform(X_dummy)\n",
    "            input_dim = X_processed.shape[1]\n",
    "            dummy_input = torch.zeros((1, input_dim), dtype=torch.float32)\n",
    "            extra_kwargs['model_input'] = dummy_input\n",
    "\n",
    "        custom_metadata = build_custom_metadata(fairness_value)\n",
    "        print(f\"Submitting {model_name} ({framework}) as {submission_type} with metadata: {custom_metadata}\")\n",
    "\n",
    "        playground.submit_model(\n",
    "            model=model,\n",
    "            preprocessor=preprocessor_obj,\n",
    "            prediction_submission=preds,\n",
    "            input_dict={\n",
    "                'description': f'Notebook submission {framework} {model_name} COMPAS_short {submission_type}',\n",
    "                'tags': f'compas_short,{framework},{submission_type}'\n",
    "            },\n",
    "            submission_type=submission_type,\n",
    "            custom_metadata=custom_metadata,\n",
    "            **extra_kwargs\n",
    "        )\n",
    "        print(\"✓ Submission succeeded.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        error_lower = str(e).lower()\n",
    "        if 'stdin' in error_lower or 'onnx' in error_lower:\n",
    "            print(f\"⊘ Skipped {model_name} due to ONNX/stdin export issue: {e}\")\n",
    "            return False\n",
    "        print(f\"✗ Submission failed: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train & Submit Scikit-learn Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sklearn-submit"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "fairness_gen = fairness_value_generator()\n",
    "\n",
    "X_train_processed = preprocessor_func(X_train)\n",
    "X_test_processed = preprocessor_func(X_test)\n",
    "\n",
    "sklearn_models = [\n",
    "    (\"LogisticRegression\", LogisticRegression(max_iter=500, random_state=42, class_weight='balanced')),\n",
    "    (\"RandomForestClassifier\", RandomForestClassifier(n_estimators=40, max_depth=10, random_state=42, class_weight='balanced')),\n",
    "]\n",
    "\n",
    "for name, model in sklearn_models:\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\"Training {name}\")\n",
    "    try:\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "            preds = (proba >= 0.5).astype(int)\n",
    "        else:\n",
    "            preds = model.predict(X_test_processed)\n",
    "        print(f\"Predictions generated: {len(preds)}; Distribution: {pd.Series(preds).value_counts().to_dict()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Training failed for {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    for submission_type in ['competition', 'experiment']:\n",
    "        fairness_val = next(fairness_gen)\n",
    "        submit_model_helper(playground, model, preprocessor, preds, 'sklearn', name, submission_type, fairness_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train & Submit Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "keras-submit"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Keras Sequential Model\")\n",
    "input_dim = X_train_processed.shape[1]\n",
    "\n",
    "keras_model = Sequential([\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "keras_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "try:\n",
    "    keras_model.fit(X_train_processed, y_train, epochs=6, batch_size=64, verbose=0, validation_split=0.1)\n",
    "    proba = keras_model.predict(X_test_processed, verbose=0).flatten()\n",
    "    keras_preds = (proba >= 0.5).astype(int)\n",
    "    print(f\"Keras predictions distribution: {pd.Series(keras_preds).value_counts().to_dict()}\")\n",
    "    for submission_type in ['competition', 'experiment']:\n",
    "        fairness_val = next(fairness_gen)\n",
    "        submit_model_helper(playground, keras_model, preprocessor, keras_preds, 'keras', 'sequential_dense', submission_type, fairness_val)\n",
    "except Exception as e:\n",
    "    print(f\"✗ Keras training/submission failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train & Submit PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pytorch-submit"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training PyTorch MLP Model\")\n",
    "input_dim = X_train_processed.shape[1]\n",
    "\n",
    "class MLPBasic(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "pytorch_model = MLPBasic(input_dim)\n",
    "\n",
    "try:\n",
    "    X_train_tensor = torch.FloatTensor(X_train_processed)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_processed)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=0.001)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    pytorch_model.train()\n",
    "    for epoch in range(6):\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = pytorch_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    pytorch_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = pytorch_model(X_test_tensor)\n",
    "        proba = torch.sigmoid(logits).numpy().flatten()\n",
    "        pytorch_preds = (proba >= 0.5).astype(int)\n",
    "    print(f\"PyTorch predictions distribution: {pd.Series(pytorch_preds).value_counts().to_dict()}\")\n",
    "\n",
    "    for submission_type in ['competition', 'experiment']:\n",
    "        fairness_val = next(fairness_gen)\n",
    "        submit_model_helper(playground, pytorch_model, preprocessor, pytorch_preds, 'pytorch', 'mlp_basic', submission_type, fairness_val)\n",
    "except Exception as e:\n",
    "    print(f\"✗ PyTorch training/submission failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Retrieve & Inspect Leaderboard\n",
    "Ensures submissions exist and prints full leaderboard for review."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "leaderboard"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    lb_data = playground.get_leaderboard()\n",
    "    if isinstance(lb_data, dict):\n",
    "        df_lb = pd.DataFrame(lb_data)\n",
    "    else:\n",
    "        df_lb = lb_data\n",
    "\n",
    "    if df_lb.empty:\n",
    "        print(\"Leaderboard is empty. (Possibly all submissions failed or were skipped.)\")\n",
    "    else:\n",
    "        print(f\"Leaderboard entries: {len(df_lb)}\")\n",
    "        if 'tags' in df_lb.columns:\n",
    "            tag_series = df_lb['tags'].astype(str)\n",
    "            print(\"Tag counts:\")\n",
    "            for t in ['compas_short', 'sklearn', 'keras', 'pytorch', 'competition', 'experiment']:\n",
    "                print(f\"  {t}: {tag_series.str.contains(t, case=False, na=False).sum()}\")\n",
    "\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1200):\n",
    "            print(df_lb.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"Failed to retrieve leaderboard: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps\n",
    "- Adjust model architectures or hyperparameters for experimentation.\n",
    "- Use different fairness metadata strategies.\n",
    "- Toggle playground visibility or extend with new frameworks.\n",
    "- Integrate automated evaluation workflows.\n",
    "\n",
    "This concludes the lightweight multi-framework COMPAS submission demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Moral Compass Challenge Extension\n",
    "The following sections extend the notebook to test creation of a new Moral Compass challenge table and simulate a user progressing through tasks and questions to obtain a final Moral Compass score.\n",
    "\n",
    "Logic adapted from `tests/test_playground_moral_compass_challenge.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resolve API Base URL & Initialize Client\n",
    "We attempt to resolve the Moral Compass API base URL using:\n",
    "1. `MORAL_COMPASS_API_BASE_URL` environment variable.\n",
    "2. `get_api_base_url()` fallback.\n",
    "\n",
    "If resolution fails, the challenge section will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mc-resolve-url"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from aimodelshare.moral_compass import MoralcompassApiClient\n",
    "from aimodelshare.moral_compass.api_client import NotFoundError, ApiClientError\n",
    "from aimodelshare.moral_compass.challenge import ChallengeManager\n",
    "from aimodelshare.moral_compass.config import get_api_base_url\n",
    "\n",
    "def resolve_api_base_url():\n",
    "    env_url = os.getenv('MORAL_COMPASS_API_BASE_URL')\n",
    "    if env_url:\n",
    "        return env_url.rstrip('/')\n",
    "    try:\n",
    "        return get_api_base_url()\n",
    "    except RuntimeError as e:\n",
    "        raise RuntimeError(\n",
    "            \"Could not resolve API base URL. Set MORAL_COMPASS_API_BASE_URL or ensure terraform outputs are accessible.\"\n",
    "        ) from e\n",
    "\n",
    "try:\n",
    "    mc_api_base_url = resolve_api_base_url()\n",
    "    print(f\"Resolved Moral Compass API base URL: {mc_api_base_url}\")\n",
    "    mc_api_client = MoralcompassApiClient(api_base_url=mc_api_base_url)\n",
    "    mc_available = True\n",
    "except Exception as e:\n",
    "    print(f\"Moral Compass API not available: {e}. Skipping challenge section.\")\n",
    "    mc_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Create / Ensure Challenge Table\n",
    "We create (idempotently) a new challenge table for a Justice & Equity themed challenge.\n",
    "Naming convention: `<playground_id>-mc` or fallback if playground ID unavailable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mc-create-table"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "if mc_available:\n",
    "    USERNAME = os.getenv('username') or input(\"Enter username for Moral Compass challenge: \") or 'notebook_user_mc'\n",
    "    base_playground_id = playground_id or 'compas_playground_notebook'\n",
    "    TABLE_ID = f\"{base_playground_id}-mc\"\n",
    "    PLAYGROUND_URL = f\"https://example.com/playground/{base_playground_id}\"\n",
    "\n",
    "    print(f\"Attempting to create/ensure table: {TABLE_ID}\")\n",
    "    try:\n",
    "        mc_api_client.create_table(\n",
    "            TABLE_ID,\n",
    "            display_name='Justice & Equity Challenge (Notebook)',\n",
    "            playground_url=PLAYGROUND_URL\n",
    "        )\n",
    "        print(\"Table creation invoked (may already exist).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Table creation skipped/failed (likely exists): {e}\")\n",
    "\n",
    "    # Confirm availability with retries\n",
    "    import time\n",
    "    max_retries = 8\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            mc_api_client.get_table(TABLE_ID)\n",
    "            print(\"Table metadata confirmed.\")\n",
    "            break\n",
    "        except (NotFoundError, ApiClientError) as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Failed to confirm table after {max_retries} attempts: {e}\")\n",
    "                mc_available = False\n",
    "            else:\n",
    "                time.sleep(0.6)\n",
    "else:\n",
    "    print(\"Skipping table creation (API unavailable).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Pre-Sync Smoke Test\n",
    "Submit a minimalist update to ensure the endpoint accepts metrics and returns a `moralCompassScore`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mc-smoke-test"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "if mc_available:\n",
    "    try:\n",
    "        smoke_resp = mc_api_client.update_moral_compass(\n",
    "            table_id=TABLE_ID,\n",
    "            username=USERNAME,\n",
    "            metrics={'accuracy': 0.5},\n",
    "            tasks_completed=0,\n",
    "            total_tasks=6,\n",
    "            questions_correct=0,\n",
    "            total_questions=14\n",
    "        )\n",
    "        assert 'moralCompassScore' in smoke_resp, 'Expected moralCompassScore in smoke response.'\n",
    "        print(f\"✓ Smoke test passed. Response: {smoke_resp}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Smoke test failed: {e}\")\n",
    "        mc_available = False\n",
    "else:\n",
    "    print(\"Skipping smoke test (API unavailable).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Build Synthetic Dataset & Select Best Model\n",
    "We reproduce a small synthetic COMPAS-like dataset, train logistic regressions with different `C` values, and select the best accuracy for reporting to the challenge."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mc-build-dataset"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def build_mc_dataset(n=200, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    race = rng.choice(['Black','White'], size=n, p=[0.5,0.5])\n",
    "    sex = rng.choice(['Male','Female'], size=n, p=[0.6,0.4])\n",
    "    age = rng.integers(18, 60, size=n)\n",
    "    priors = rng.integers(0, 15, size=n)\n",
    "    base = 0.3 + 0.03*priors + (race == 'Black')*0.05 + (sex == 'Male')*0.02\n",
    "    prob = 1/(1+np.exp(-(base - 0.5)))\n",
    "    label = (rng.random(n) < prob).astype(int)\n",
    "    df = pd.DataFrame({'race':race,'sex':sex,'age':age,'priors':priors,'label':label})\n",
    "    return df\n",
    "\n",
    "def featurize_mc(df):\n",
    "    d = df.copy()\n",
    "    d['race_Black'] = (d['race']=='Black').astype(int)\n",
    "    d['sex_Male'] = (d['sex']=='Male').astype(int)\n",
    "    X = d[['age','priors','race_Black','sex_Male']]\n",
    "    y = d['label']\n",
    "    return X, y\n",
    "\n",
    "def train_lr_variant(X, y, C):\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, random_state=13)\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lr', LogisticRegression(C=C, max_iter=200))\n",
    "    ])\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    acc = pipe.score(Xte, yte)\n",
    "    return pipe, acc\n",
    "\n",
    "if mc_available:\n",
    "    df_mc = build_mc_dataset()\n",
    "    X_mc, y_mc = featurize_mc(df_mc)\n",
    "    candidate_Cs = [0.1, 1.0, 3.0]\n",
    "    best_acc = -1.0\n",
    "    best_C = None\n",
    "    for C in candidate_Cs:\n",
    "        modelC, accC = train_lr_variant(X_mc, y_mc, C)\n",
    "        print(f\"C={C} | Accuracy={accC:.4f}\")\n",
    "        if accC > best_acc:\n",
    "            best_acc = accC\n",
    "            best_C = C\n",
    "    print(f\"Best C: {best_C} with accuracy={best_acc:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping model selection (API unavailable).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Initialize Challenge Manager & Set Metric\n",
    "We record the best accuracy as the primary metric for the user in the challenge."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mc-init-manager"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from aimodelshare.moral_compass.challenge import ChallengeManager\n",
    "\n",
    "if mc_available:\n",
    "    try:\n",
    "        manager = ChallengeManager(table_id=TABLE_ID, username=USERNAME, api_client=mc_api_client)\n",
    "        manager.set_metric('accuracy', best_acc, primary=True)\n",
    "        print(f\"Primary metric 'accuracy' set to {best_acc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize ChallengeManager: {e}\")\n",
    "        mc_available = False\n",
    "else:\n",
    "    print(\"Skipping manager initialization (API unavailable).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Progress Through Tasks & Questions\n",
    "We iterate through all tasks, completing them and answering each question with the correct index. After each task block, we sync with the server and monitor score progression."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mc-progress"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "if mc_available:\n",
    "    try:\n",
    "        challenge = manager.challenge\n",
    "        prev_score = 0.0\n",
    "        for task in challenge.tasks:\n",
    "            print(f\"\\nTask: {task.id} - Completing and answering questions...\")\n",
    "            manager.complete_task(task.id)\n",
    "            for q in task.questions:\n",
    "                manager.answer_question(task.id, q.id, selected_index=q.correct_index)\n",
    "            sync_resp = manager.sync()\n",
    "            mc_score = sync_resp.get('moralCompassScore', 0)\n",
    "            print(f\"Synced. Moral Compass Score: {mc_score:.4f}\")\n",
    "            if mc_score + 1e-9 < prev_score:\n",
    "                print(f\"Warning: Score decreased from {prev_score:.4f} to {mc_score:.4f}\")\n",
    "            prev_score = mc_score\n",
    "        print(\"\\nAll tasks completed and synced.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during task progression: {e}\")\n",
    "        mc_available = False\n",
    "else:\n",
    "    print(\"Skipping task progression (API unavailable).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Final Summary & Leaderboard Validation\n",
    "We retrieve the user's progress summary and verify leaderboard entry alignment with local score preview."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mc-final-summary"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "if mc_available:\n",
    "    try:\n",
    "        summary = manager.get_progress_summary()\n",
    "        print(\"Progress Summary:\")\n",
    "        for k, v in summary.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        assert summary['tasksCompleted'] == summary['totalTasks'], 'Not all tasks completed.'\n",
    "        assert summary['questionsCorrect'] == summary['totalQuestions'], 'Not all questions answered correctly.'\n",
    "        final_local = summary.get('localScorePreview', 0)\n",
    "        assert final_local > 0, 'Final local score should be positive.'\n",
    "\n",
    "        lb = mc_api_client.list_users(TABLE_ID, limit=100)\n",
    "        entries = [u for u in lb.get('users', []) if u.get('username') == USERNAME]\n",
    "        if not entries:\n",
    "            print(\"User not found on leaderboard.\")\n",
    "        else:\n",
    "            user_entry = entries[0]\n",
    "            print(\"\\nLeaderboard Entry:\")\n",
    "            for k, v in user_entry.items():\n",
    "                print(f\"  {k}: {v}\")\n",
    "            mc_score_lb = user_entry.get('moralCompassScore', 0)\n",
    "            if mc_score_lb + 0.2 < final_local:\n",
    "                print(\"Leaderboard score appears misaligned with local preview.\")\n",
    "            else:\n",
    "                print(\"✓ Leaderboard score aligned sufficiently with local preview.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Final summary/leaderboard validation failed: {e}\")\n",
    "else:\n",
    "    print(\"Skipping final summary (API unavailable).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Challenge Section Complete\n",
    "You have (if API was available) created a challenge table, submitted metric progress, completed tasks & questions, and validated the leaderboard entry.\n",
    "\n",
    "Feel free to modify:\n",
    "- Hyperparameter candidates\n",
    "- Additional metrics\n",
    "- Custom scoring logic\n",
    "- Partial task completion for experimental flows\n",
    "\n",
    "End of notebook."
   ]
  }
 ]
}
